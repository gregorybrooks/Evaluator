import org.json.simple.JSONArray;
import org.json.simple.JSONObject;
import org.json.simple.parser.JSONParser;
import org.json.simple.parser.ParseException;

import java.io.*;
import java.util.*;

/**
 */
public class Evaluator {

//    private static final String resultsFilesDirectory = "/home/glbrooks/BETTER/";
//    private static final String metadataFilesDirectory = "/home/glbrooks/BETTER/tools/ConvertDryRunTasks/test_data/";

    private static final String resultsFilesDirectory = "c:\\Users\\grego\\BETTER\\runfiles\\";
    private static final String metadataFilesDirectory = "c:\\Users\\grego\\BETTER\\taskfiles\\";

    /**
     * The shorthand names for the query formulation solutions generated by ConvertDryRunTasks,
     * plus names for the other player's solutions that I copied to the same directory, using
     * the naming convention. This allows us to evaluate them, too.
     */
    private final List<String> solutionNames = new ArrayList<String>(Arrays.asList(
            "CLEAR-BASE-TEST",
            "CLEAR-1-TEST",
            "CLEAR-2-TEST",
            "CLEAR-3-TEST",
            "CLEAR-4-TEST",
            "CLEAR-5-TEST",
            "CLEAR-6-TEST",
            "CLEAR-7-TEST",
            "BBN-1",
            "JHU-1",
            "BROWN-1"
            ));

    /**
     * The file that contains the task and request definitions.
     */
    private static final String runType = "auto";
    private static final String tasksAndRequestsFile = metadataFilesDirectory + "dry-run-topics."
        + runType + ".json";

    /**
     * The tasks in the tasksAndRequestsFile converted into a Map of Task objects.
     */
    private final Map<String,Task> tasks = new HashMap<>();

    /**
     * The solution results after being converted to Solution objects.
     */
    private final Map<String,Solution> solutionResults = new HashMap<>();

    private Integer resultSetSize = -1;  // number of hits to look at for each request

    /**
     * Within the solution are multiple Tasks, and each Task has multiple Requests.
     * This class represents the results of running the query for a Request.
     */
    private class RequestRun {
        private String queryID;
        private List<String> docids;
        RequestRun(String queryID, List<String> docids) {
            this.queryID = queryID;
            this.docids = docids;
        }
        String getQueryID() { return queryID; }
        List<String> getDocids() { return docids; }
        public String toString() {
            return "Request " + queryID + ": " + docids;
        }
    }

    /**
     * Represents the results of a particular query formulation's Galago execution,
     * or of another player's results.
     */
    private class Solution {
        private List<RequestRun> requestRuns;
        private String name;
        Solution (String name, List<RequestRun> requestRuns) {
            this.requestRuns = requestRuns;
            this.name = name;
        }
        List<RequestRun> getRequestRuns() {
            return requestRuns;
        }
        String getName() {
            return name;
        }
        public String toString() {
            return "Solution " + name + ": " + requestRuns;
        }
    }

    /**
     * Reads the Dry Run JSON file containing the analytic tasks and requests to be processed
     * and constructs a Map of Tasks and Requests that represent them.
     * @throws IOException
     * @throws ParseException
     */
    private void loadTasks() throws IOException, ParseException {
        Reader reader = new BufferedReader(new InputStreamReader(
                new FileInputStream(tasksAndRequestsFile)));
        JSONParser parser = new JSONParser();
        JSONArray tasksJSON = (JSONArray) parser.parse(reader);
        for (Object oTask : tasksJSON) {
            Task t = new Task((JSONObject) oTask);  // this gets Requests, too
            tasks.put(t.taskNum, t);
        }
    }

    /**
     * Returns the Task object for a given task name.
     * @param taskNum The Task name, like "DR-T1"
     * @return Returns the Task object.
     */
    private Task findTask(String taskNum) {
        return tasks.get(taskNum);
    }

    /**
     * Returns the Request object for a given request.
     * @param t The Task this request belongs to.
     * @param requestNum The name of the request, like "DR-T1-r1".
     * @return Returns the Request object.
     */
    private Request findRequest(Task t, String requestNum) {
        return t.requests.get(requestNum);
    }

    /**
     * Filters certain characters that cause problems for the Galago query.
     * @param q The string to be filtered.
     * @return Returns the filtered string.
     */
    public static String filterCertainCharacters(String q) {
        if (q == null || q.length() == 0) {
            return q;
        }
        else {
            q = q.replaceAll("\\.", "");  // with the periods the queries hang
            q = q.replaceAll("\\(", "");  // parentheses are included in the token
            q = q.replaceAll("\\)", "");  // which causes that term to not be matched
            //          q = q.replaceAll("\u2019", "'");  // this single-quote is in the original doc
            return q;
        }
    }

    /**
     * Reads in the query results files for all the solutions in solutionNames.
     * The solutions are converted into Solution objects and stored into a map
     * at solutionResults. Key is the solution name, e.g. "CLEAR-1".
     */
    private void loadSolutionResults() {
        for (String s : solutionNames) {
            Solution solution = readQueryResultsFile(s);
            solutionResults.put(s, solution);
        }
    }

    /**
     * Reads in the file that was output from Galago's batch-search function,
     * or from another player's retrieval.
     * The file is a TREC format as required by the Dry Run.
     * @param solution The name of the solution, e.g. "CLEAR-1".
     * @return A Solution object that captures the retrieval results.
     */
    private Solution readQueryResultsFile(String solution)  {
        String fileName = resultsFilesDirectory + "dry-run-topics." + runType
                 + "." + solution + ".out";
        List<RequestRun> requestRuns = new ArrayList<>();
        List<String> docids = new ArrayList<>();
        try (BufferedReader br = new BufferedReader(new FileReader(fileName))) {
            String line;
            String prevQueryID = "NONE";
            int docidsAdded = 0;
            while ((line = br.readLine()) != null) {
                String queryID = line.split("[ \t]+")[0];
                if (!prevQueryID.equals(queryID)) {
                    if (!prevQueryID.equals("NONE")) {
                        // Clone the list
                        List<String> cloned_list
                                = new ArrayList<String>(docids);
                        RequestRun r = new RequestRun(prevQueryID, cloned_list);
                        requestRuns.add(r);
                        docids.clear();
                        docidsAdded = 0;
                    }
                }
                prevQueryID = queryID;
                if (docidsAdded < resultSetSize) {
                    docids.add(line.split("[ \t]+")[2]); // doc ID is 3rd field
                    ++docidsAdded;
                }
            }
            if (!prevQueryID.equalsIgnoreCase("")) {
                // Clone the list
                List<String> cloned_list
                        = new ArrayList<String>(docids);
                RequestRun r = new RequestRun(prevQueryID, cloned_list);
                requestRuns.add(r);
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
        return new Solution(solution, requestRuns);
    }

    /**
     * Evaluates the solutions and output a CSV file of evaluation results.
     * Evaluation Type 1 (E1) uses the request-level hint docs only as the known relevant
     * docs for a request. Evaluation Type 2 (E2) uses all of the task-level hint docs
     * and all of the request-level hint docs of all of the requests for this task
     * as the known relevant docs for a request.
     *
     * For averaging the evaluation results, we use two approaches:
     * MICRO just accumulates all of the request-level results and divides them by
     * the total number of requests.
     * MACRO averages the results at the task level, accumulates those results and
     * divides the totals by the number of tasks.
     * @throws IOException
     * @throws InterruptedException
     * @throws ParseException
     */
    private void process() throws IOException, InterruptedException, ParseException {
        Pathnames.getPathnames();
        AnalyticTasks tasks = new AnalyticTasks();
        tasks.fixTaskDocs();  //Make sure task-docs contains all req-docs for that task

        /* This is the list of solutions to process.
        These should be the solutions that did NOT reject the task-docs
        and request-docs. These are the "...-TEST" versions for us, and for the other
        player's files which apparently did not reject the hint docs anyway I copied them
        to the same directory and used our naming convention, so we can evaluate them, too.
         */
        List<String> solutions = new ArrayList<String>(Arrays.asList(
                "CLEAR-BASE-TEST",
                "CLEAR-2-TEST",
                "CLEAR-1-TEST",
                "CLEAR-3-TEST",
                "CLEAR-4-TEST",
                "CLEAR-5-TEST",
                "CLEAR-6-TEST",
                "CLEAR-7-TEST",
                "CLEAR-8-TEST",
                "BBN-1",
                "JHU-1",
                "BROWN-1"
                ));
        /*
        This is the list of result-set sizes we want to evaluate, for each solution.
        We calculate recall and precision for the top N hits, where N is each of the sizes
        in this list.
         */
        List<Integer> resultSetSizes = new ArrayList<Integer>(Arrays.asList(
                10, 20, 50, 100, 250, 500, 1000
        ));

        /* Create and open the output CSV file */
        FileWriter csvWriter = new FileWriter("c:\\Users\\grego\\comparison.csv");
        /* Write the header line */
        csvWriter.append("Solution");
        csvWriter.append(",");
        csvWriter.append("Judgment Set Used");
        csvWriter.append(",");
        csvWriter.append("Result Set Size");
        csvWriter.append(",");
        /*
        csvWriter.append("Averaging Type");
        csvWriter.append(",");
        */
        csvWriter.append("Recall (Pct)");
        csvWriter.append(",");
        csvWriter.append("Precision (Pct)");
        csvWriter.append(",");
        csvWriter.append("R Precision (Pct)");
        csvWriter.append(",");
        csvWriter.append("Unjudged (Pct)");
        csvWriter.append("\n");

        for (Integer rsize : resultSetSizes) {
            resultSetSize = rsize;  // set the instance var
            for (String solutionName : solutions) {
                QueryFormulation s1 = new QueryFormulation(solutionName);
                System.out.println("Evaluating solution " + s1.getName() + " for top "
                    + resultSetSize + " results");
                List<String> requestIDs = tasks.getRequestIDs();
                ListIterator<String> requestIDIterator = requestIDs.listIterator();

                /* macro averaging approach accumulators */
                double totalTaskE1Recall = 0.0;
                double totalTaskE1Precision = 0.0;
                double totalTaskE1RPrecision = 0.0;
                double totalTaskE1Unjudged = 0.0;
                double totalTaskE2Recall = 0.0;
                double totalTaskE2Precision = 0.0;
                double totalTaskE2RPrecision = 0.0;
                double totalTaskE2Unjudged = 0.0;

                int numRequests = requestIDs.size();
                int totalTasks = 0;  // this will be calculated as we go through the requests
                String prevTaskID = "EMPTY";  // used to detect Task changes
                String taskID = "";  // used to detect Task changes
                int totalRequestsInTask = 0;  // this will be calculated as we go through the requests

                /* macro averaging approach task-level accumulators */
                double taskE1Recall = 0.0;
                double taskE1Precision = 0.0;
                double taskE1RPrecision = 0.0;
                double taskE1Unjudged = 0.0;
                double taskE2Recall = 0.0;
                double taskE2Precision = 0.0;
                double taskE2RPrecision = 0.0;
                double taskE2Unjudged = 0.0;

                /* Assumption: all requests for a task are contiguous as we iterate them */
                while (requestIDIterator.hasNext()) {
                    String requestID = requestIDIterator.next();
                    List<String> s1DocidsList = s1.getDocids(requestID, rsize);
                    taskID = requestID.substring(0, 5);

                    if (!taskID.equals("DR-T1"))  //TEMP
                        continue;  // TEMP

                    if (!taskID.equals(prevTaskID) && !prevTaskID.equals("EMPTY")) {
                        ++totalTasks;
                        totalTaskE1Recall += (taskE1Recall / totalRequestsInTask);
                        totalTaskE1Precision += (taskE1Precision / totalRequestsInTask);
                        totalTaskE1RPrecision += (taskE1RPrecision / totalRequestsInTask);
                        totalTaskE1Unjudged += (taskE1Unjudged / totalRequestsInTask);
                        totalTaskE2Recall += (taskE2Recall / totalRequestsInTask);
                        totalTaskE2Precision += (taskE2Precision / totalRequestsInTask);
                        totalTaskE2RPrecision += (taskE2RPrecision / totalRequestsInTask);
                        totalTaskE2Unjudged += (taskE2Unjudged / totalRequestsInTask);
                        taskE1Recall = 0.0;
                        taskE1Precision = 0.0;
                        taskE1RPrecision = 0.0;
                        taskE1Unjudged = 0.0;
                        taskE2Recall = 0.0;
                        taskE2Precision = 0.0;
                        taskE2RPrecision = 0.0;
                        taskE2Unjudged = 0.0;
                        totalRequestsInTask = 0;
                    }
                    prevTaskID = taskID;
                    ++totalRequestsInTask;

                    List<String> reqDocList = tasks.getRequestRelevantDocids(requestID);
                    List<String> taskDocList = tasks.getTaskAndRequestRelevantDocids(requestID);
                    int taskDocsFound = 0;
                    int requestDocsFound = 0;
                    int unjudgedDocsFound = 0;
                    for (String s : taskDocList) {
                        if (s1DocidsList.contains(s)) {
                            ++taskDocsFound;
                        }
                    }
                    for (String s : reqDocList) {
                        if (s1DocidsList.contains(s)) {
                            ++requestDocsFound;
                        }
                    }
                    for (String s : s1DocidsList) {
                        if (tasks.getRelevanceJudgement(requestID, s) == null) {
                            ++unjudgedDocsFound;
                        }
                    }


                    /* E1 is request level, E2 is task level */
                    double e1Recall = ((double) requestDocsFound / reqDocList.size()) * 100;
                    double e2Recall = ((double) taskDocsFound / taskDocList.size()) * 100;
                    double e1Precision = ((double) requestDocsFound / s1DocidsList.size()) * 100;
                    double e2Precision = ((double) taskDocsFound / s1DocidsList.size()) * 100;
                    double e1Unjudged = ((double) unjudgedDocsFound / s1DocidsList.size()) * 100;
                    double e2Unjudged = ((double) unjudgedDocsFound / s1DocidsList.size()) * 100;

                    String[] s1Docids = new String[s1DocidsList.size()];
                    s1Docids = s1DocidsList.toArray(s1Docids);

                    /* How many request docs are in the top N hits? (E1) */
                    int docMatches = 0;
                    for (int x = 0; x < reqDocList.size(); ++x) {
                        //System.out.println(s1Docids[x]);
                        if (reqDocList.contains(s1Docids[x])) {
                            docMatches += 1;
                        }
                    }
                    double e1RPrecision = ((double) docMatches / reqDocList.size()) * 100;

                    /* How many task docs are in the top N hits? (E2) */
                    docMatches = 0;
                    for (int x = 0; x < taskDocList.size(); ++x) {
                        //System.out.println(s1Docids[x]);
                        if (taskDocList.contains(s1Docids[x])) {
                            docMatches += 1;
                        }
                    }
                    double e2RPrecision = ((double) docMatches / taskDocList.size()) * 100;

                    /* Accumulators for macro averaging approach */
                    taskE1Precision += e1Precision;
                    taskE2Precision += e2Precision;
                    taskE1Recall += e1Recall;
                    taskE2Recall += e2Recall;
                    taskE1RPrecision += e1RPrecision;
                    taskE2RPrecision += e2RPrecision;
                    taskE1Unjudged += e1Unjudged;
                    taskE2Unjudged += e2Unjudged;

                }
                /* Flush out that last Task */
                if (!prevTaskID.equals("EMPTY")) {
                    ++totalTasks;
                    totalTaskE1Recall += (taskE1Recall / totalRequestsInTask);
                    totalTaskE1Precision += (taskE1Precision / totalRequestsInTask);
                    totalTaskE1RPrecision += (taskE1RPrecision / totalRequestsInTask);
                    totalTaskE1Unjudged += (taskE1Unjudged / totalRequestsInTask);
                    totalTaskE2Recall += (taskE2Recall / totalRequestsInTask);
                    totalTaskE2Precision += (taskE2Precision / totalRequestsInTask);
                    totalTaskE2RPrecision += (taskE2RPrecision / totalRequestsInTask);
                    totalTaskE2Unjudged += (taskE2Unjudged / totalRequestsInTask);
                }

                double macroAvgE1Recall = totalTaskE1Recall / totalTasks;
                double macroAvgE1Precision = totalTaskE1Precision / totalTasks;
                double macroAvgE1RPrecision = totalTaskE1RPrecision / totalTasks;
                double macroAvgE1Unjudged = totalTaskE1Unjudged / totalTasks;
                double macroAvgE2Recall = totalTaskE2Recall / totalTasks;
                double macroAvgE2Precision = totalTaskE2Precision / totalTasks;
                double macroAvgE2RPrecision = totalTaskE2RPrecision / totalTasks;
                double macroAvgE2Unjudged = totalTaskE2Unjudged / totalTasks;

                csvWriter.append(solutionName);
                csvWriter.append(",");
                csvWriter.append("REQUEST RELEVANT");
                csvWriter.append(",");
                csvWriter.append(Integer.toString(resultSetSize));
                csvWriter.append(",");
                csvWriter.append(String.format("%.2f", macroAvgE1Recall));
                csvWriter.append(",");
                csvWriter.append(String.format("%.2f", macroAvgE1Precision));
                csvWriter.append(",");
                csvWriter.append(String.format("%.2f", macroAvgE1RPrecision));
                csvWriter.append(",");
                csvWriter.append(String.format("%.2f", macroAvgE1Unjudged));
                csvWriter.append("\n");

                csvWriter.append(solutionName);
                csvWriter.append(",");
                csvWriter.append("TASK OR REQUEST RELEVANT");
                csvWriter.append(",");
                csvWriter.append(Integer.toString(resultSetSize));
                csvWriter.append(",");
                csvWriter.append(String.format("%.2f", macroAvgE2Recall));
                csvWriter.append(",");
                csvWriter.append(String.format("%.2f", macroAvgE2Precision));
                csvWriter.append(",");
                csvWriter.append(String.format("%.2f", macroAvgE2RPrecision));
                csvWriter.append(",");
                csvWriter.append(String.format("%.2f", macroAvgE2Unjudged));
                csvWriter.append("\n");

                csvWriter.flush();

            }
        }
        csvWriter.close();
    }

    /**
     * Public entry point for the program.
     * @param args No arguments are expected.
     */
    public static void main(String[] args) throws IOException, InterruptedException, ParseException {
        Evaluator evaluator = new Evaluator();
        evaluator.process();
    }
}